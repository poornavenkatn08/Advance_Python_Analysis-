# Data Analysis Toolkit ðŸ”

A comprehensive Python toolkit for web scraping, data cleaning, and exploratory data analysis (EDA).  
This toolkit provides professional-grade tools for the complete data analysis pipeline with real-world tested capabilities and proven performance metrics.

[![Python Version](https://img.shields.io/badge/python-3.7%2B-blue.svg)](https://python.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code Style: Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

Data Analysis Toolkit ðŸ”
A modular Python framework designed for end-to-end data processing, including targeted web scraping, automated data cleaning, and comprehensive exploratory data analysis (EDA).

This toolkit is built with a controller-logic architecture, allowing individual modules to run independently or as a unified pipeline via a master script.

ðŸ—ï¸ System Architecture
The toolkit is divided into three specialized engines, each validated through real-world data use cases. It follows an ETL (Extract, Transform, Load) pattern where data flows seamlessly from the web into a cleaned state, followed by statistical profiling.

1. Web Extraction Engine (webscraper.py)

Capability: Parses and extracts structured data from complex HTML environments using BeautifulSoup4.

Applied Use Case: Configured for Fortune 500 Financial Data, extracting the largest US companies by revenue from Wikipedia and converting financial string notations into analysis-ready numeric formats.

2. Data Engineering Pipeline (data_Cleaner.py)

Capability: An OOP-based ETL pipeline that implements Regex-driven standardization and business logic validation.

Applied Use Case: Validated on Tech Layoffs & Contact Datasets, utilizing automated deduplication and phone/address parsing to achieve a 99.8% data retention rate.

3. Automated EDA Suite (eda_analyzer.py)

Capability: A statistical engine that generates high-fidelity reports and visualizations using Seaborn and Matplotlib.

Analysis Suite: Performs automated outlier detection (IQR method), identifies strong correlations (âˆ£râˆ£>0.7), and calculates distribution metrics (Skewness/Kurtosis).

ðŸš€ The Master Pipeline
The project features a master_pipeline.py script that automates the entire lifecycle. It handles directory creation, manages data flow between modules, and generates a final executive report.

Pipeline Workflow:

Scrape: Extracts raw corporate data into the /data folder.

Clean: Processes raw CSVs (e.g., Layoffs data), standardizes formats, and outputs "Cleaned" versions.

Analyze: Consumes the cleaned data to produce statistical summaries and visual plots in the /reports folder.

ðŸ“ Project Structure
Bash
Data_Analysis_Toolkit/
â”œâ”€â”€ master_pipeline.py       # Master Controller Script
â”œâ”€â”€ webscraper.py            # Extraction Module (Applied to Fortune 500)
â”œâ”€â”€ data_Cleaner.py          # ETL Module (Applied to Layoffs/Contact data)
â”œâ”€â”€ eda_analyzer.py          # EDA Module (Statistical Profiling)
â”œâ”€â”€ data/                    # Storage for Raw and Processed CSVs
â””â”€â”€ reports/                 # Automated PDF/Text analysis reports
ðŸ› ï¸ Installation & Usage
1. Requirements

Ensure you have the following libraries installed:

Bash
pip install pandas requests beautifulsoup4 seaborn matplotlib numpy scipy
2. Running the Full Pipeline

To execute all modules at once and generate a final report:

Bash
python master_pipeline.py

## ðŸ“‹ Requirements

- Python 3.7+  
- pandas  
- numpy  
- matplotlib  
- seaborn  
- requests  
- beautifulsoup4  
- scipy (for advanced statistics)  
- logging  

---

## ðŸ› ï¸ Installation

### Option 1: Clone Repository
```bash
git clone https://github.com/yourusername/data-analysis-toolkit.git
cd data-analysis-toolkit
pip install -r requirements.txt
```

### Option 2: Install from PyPI (Future Release)
```bash
pip install data-analysis-toolkit
```

---

## ðŸŽ¯ Quick Start

### Web Scraping
```python
from src.webscraper import DataScraper

# Initialize scraper
scraper = DataScraper()

# Scrape Fortune 500 data
fortune_data = scraper.scrape_fortune_500("fortune_500_2024.csv")

# Scrape demographic data
demo_data = scraper.scrape_demographics("world_population_2024.csv")

# Display summary
scraper.display_summary()
```

### Data Cleaning
```python
from src.data_cleaner import AdvancedDataCleaner

# Initialize cleaner
cleaner = AdvancedDataCleaner()

# Clean your data with high retention rate
cleaner.clean_all("raw_data.csv")
cleaner.save_cleaned_data("cleaned_data.csv")

# Generate detailed cleaning report
report = cleaner.generate_cleaning_report()
print(f"Data retention rate: {report['retention_rate']}%")
```

### Exploratory Data Analysis
```python
from src.eda_analyzer import ComprehensiveEDA

# Initialize EDA analyzer
eda = ComprehensiveEDA()

# Load and analyze data
eda.load_data("your_dataset.csv")
eda.run_complete_eda()

# Generate comprehensive report with statistics
report = eda.generate_comprehensive_report("eda_report.txt")

# Access specific analysis results
correlations = eda.get_strong_correlations(threshold=0.7)
outliers = eda.detect_outliers_iqr()
missing_analysis = eda.analyze_missing_values()
```

---

## ðŸ“ˆ Analysis Capabilities

### Statistical Analysis
- **Descriptive Statistics**: Mean, median, std, skewness, kurtosis  
- **Correlation Analysis**: Pearson correlation with strong correlation detection  
- **Distribution Analysis**: Shape analysis and normality testing  
- **Outlier Detection**: IQR method with reporting and percentages  

### Data Quality Assessment
- **Missing Value Analysis**: Breakdown by column with percentages  
- **Duplicate Detection**: Efficient identification and removal  
- **Data Type Optimization**: Automatic type inference & memory optimization  
- **Categorical Analysis**: Unique values and distributions  

### Advanced Features
- **Memory Efficient**: Minimal footprint for large datasets  
- **Batch Processing**: Handle multiple files simultaneously  
- **Custom Business Rules**: Domain-specific validation logic  
- **Export Flexibility**: Multiple formats & custom reports  

---

## ðŸ“Š Example Use Cases

1. **Global Demographics Analysis**: Population data, growth trends, outliers  
2. **Corporate Layoffs Analysis**: Employment data, industry trends, time patterns  
3. **Market Research**: Competitor scraping, contact standardization, analysis  
4. **Academic Research**: Clean survey data, statistical analysis, publishable reports  
5. **Business Intelligence**: Automate collection, ensure quality, dashboards  

---

## ðŸ“ Project Structure

```
data-analysis-toolkit/
â”œâ”€â”€ src/                    # Main source code
â”‚   â”œâ”€â”€ webscraper.py       # Web scraping utilities
â”‚   â”œâ”€â”€ data_cleaner.py     # Advanced data cleaning pipeline
â”‚   â””â”€â”€ eda_analyzer.py     # Comprehensive EDA & visualization
â”œâ”€â”€ examples/               # Real usage examples & tutorials
â”œâ”€â”€ data/                   
â”‚   â”œâ”€â”€ raw/                # Raw scraped data
â”‚   â”œâ”€â”€ processed/          # Cleaned data (99.8% retention)
â”‚   â””â”€â”€ outputs/            # Reports & visualizations
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ tests/                  # Unit tests with real scenarios
â””â”€â”€ notebooks/              # Jupyter notebooks
```

---

## ðŸ” Sample Output

### Data Cleaning Report
```
============================================================
DATA CLEANING REPORT
============================================================
Original rows: 2361
Duplicates removed: 5
Do not contact removed: 0
Invalid phones removed: 0
Final rows: 2356
Data retention rate: 99.8%
============================================================
```

### EDA Analysis Summary
```
============================================================
DATASET OVERVIEW
============================================================
Shape: 234 rows Ã— 19 columns
Memory Usage: 0.07 MB
Missing Values: 15
Duplicate Rows: 0
Strong Correlations Found: 31 pairs (|r| > 0.7)
Outliers Detected: 19-35 per column (IQR method)
============================================================
```

---

## ðŸ§ª Testing

Run the comprehensive test suite:
```bash
python -m pytest tests/ -v
```

Run performance benchmarks:
```bash
python tests/benchmark_performance.py
```

---

## ðŸ“š Documentation

- [Installation Guide](https://github.com/poornavenkatn08/Python_Pandas-Data-Analysis-Portfolio/blob/main/docs/Installation.md)  
- [Usage Examples with Real Data](https://github.com/poornavenkatn08/Python_Pandas-Data-Analysis-Portfolio/blob/main/docs/usage.md)  
- [API Reference](https://github.com/poornavenkatn08/Python_Pandas-Data-Analysis-Portfolio/blob/main/docs/api_reference.md)  

---

## Development Setup

1. Fork the repository  
2. Create a virtual environment: `python -m venv venv`  
3. Activate venv: `source venv/bin/activate` (Linux/Mac) or `venv\Scriptsctivate` (Windows)  
4. Install dependencies: `pip install -r requirements.txt`  
5. Install dev dependencies: `pip install -r requirements-dev.txt`  
6. Run tests: `python -m pytest tests/`  
7. Run benchmarks: `python tests/benchmark_performance.py`  

---

## ðŸŽ¯ Performance Guarantees

- **Data Cleaning**: 99%+ retention rate on real datasets  
- **EDA Processing**: < 2 seconds for datasets up to 500 rows Ã— 20 columns  
- **Memory Usage**: < 100MB for typical datasets  
- **Accuracy**: scipy-backed statistical analysis  

---

## ðŸ“œ License

This project is licensed under the MIT License â€“ see the [LICENSE](LICENSE) file for details.

---

## ðŸ™ Acknowledgments

- Wikipedia and open data sources  
- Python community (pandas, numpy, scipy)  
- Contributors and testers  
- Open data initiatives  

---

## ðŸ”„ Changelog

### v1.0.0 (Current)
- âœ… Proven data cleaning with 99.8% retention  
- âœ… Comprehensive EDA for multi-dimensional datasets  
- âœ… Advanced statistical analysis with correlation detection  
- âœ… Efficient outlier detection (IQR method)  
- âœ… Memory-optimized processing  
- âœ… Professional logging & error handling  
- âœ… Real-world tested on demographics & employment data  
## ðŸ“¬ Contact

Letâ€™s connect! I'm open to collaboration and job opportunities in data analytics and visualization.

ðŸ“§ pvneelakantam@gmail.com
ðŸ”— https://www.linkedin.com/in/pneelakantam/
